---
title: "Lab6"
format: html
editor: visual
---

#Question 1: Download data
##Set Up

```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(patchwork)
```

```{r}
root  <- 'https://gdex.ucar.edu/dataset/camels/file'
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf', 
              mode = 'wb')

```
```{r}
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')

walk2(remote_files, local_files, download.file, quiet = TRUE)

# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE)

# merge the data into a single data frame
camels <- reduce(camels, power_full_join, by = 'gauge_id')

```

```{r}
list.files("data")
```
###yay it worked!  all 6 data files + the PDF are here

#Question 2:Make 2 maps

```{r}
p1 <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()

p1
```

###It looks just like the map in the assignment!  
```{r}
# First map: Color points by aridity
p2 <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = aridity)) +
  scale_color_gradient(low = "pink", high = "navy") +
  labs(color = "Aridity Index") +
  ggthemes::theme_map()

# Second map: Color points by p_mean (mean annual precipitation)
p3 <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = p_mean)) +
  scale_color_gradient(low = "lightblue", high ="purple") +
  labs (color = "p_mean (mm)") +
  ggthemes::theme_map()


p2 + p3
```


##Model Preparation

```{r}
camels |> 
  select(aridity, p_mean, q_mean) |> 
  drop_na() |> 
  cor()
```
###Looks like the expected results from the assignments. 

```{r}
# Create a scatter plot of aridity vs rainfall
ggplot(camels, aes(x = aridity, y = p_mean)) +
  # Add points colored by mean flow
  geom_point(aes(color = q_mean)) +
  # Add a linear regression line
  geom_smooth(method = "lm", color = "red", linetype = 2) +
  # Apply the viridis color scale
  scale_color_viridis_c() +
  # Add a title, axis labels, and theme (w/ legend on the bottom)
  theme_linedraw() + 
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```
###The relationship is seen here just like in the assignment.   There is a relationship between rainfall but it is  not linear.

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c() +
  # Apply log transformations to the x and y axes
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```
###Log scaled made a more leniar relationship, but clustared and unevenly distributed.

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  # Apply a log transformation to the color scale
  scale_color_viridis_c(trans = "log") +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom",
        # Expand the legend width ...
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(.5, "cm")) + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow") 
```
###Definitely more evenly spread after being skewed, just like expected.


##Model building
###set seed for reproducabilty:
```{r}
set.seed(123)
# Bad form to perform simple transformations on the outcome variable within a 
# recipe. So, we'll do it here.
camels <- camels |> 
  mutate(logQmean = log(q_mean))

# Generate the split
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)
###use 80% of the data for training and 20% for testing with no stratification.
camels_cv <- vfold_cv(camels_train, v = 10)
### 10-fold cross validation dataset to help us evaluate multi-model setups.
```

###Separately we have used the recipe function to define a series of data preprocessing steps:

```{r}
# Create a recipe to preprocess the data
rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())
```

###fitting a linear model to the data. 

```{r}
# Prepare the data
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

# Interaction with lm
#  Base lm sets interaction terms with the * symbol
lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)
summary(lm_base)
```

###Just as expected

```{r}
# Sanity Interaction term from recipe ... these should be equal!!
summary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))
```


###Just as expected again

##A Model's ability to predict new data (incorrect tests)
###Using the wrong verion augment function to add predicted values to the test data won't work because the recipe has not been applied

```{r}
nrow(camels_test)
nrow(camels_train)

```
###look how many more test vs train

```{r}
#broom::augment(lm_base, data = camels_test)
```
###As expected, there is an erro, I will put it behind a "#" so that I can render this assignment.

###Using the predict function to directly test the data without recipe onject we also see issues

```{r}
camels_test$p2 = predict(lm_base, newdata = camels_test)

## Scales way off!
ggplot(camels_test, aes(x = p2, y = logQmean)) + 
  geom_point() + 
  # Linear fit line, no error bands
  geom_smooth(method = "lm", se = FALSE, size =1) +
  # 1:1 line
  geom_abline(color = "red", size = 1) + 
  labs(title = "Linear Model Using `predict()`",
       x = "Predicted Log Mean Flow",
       y = "Observed Log Mean Flow") + 
  theme_linedraw()
```
###As expected.

##Correct version
###prep -> bake -> predict; using the prep and bake functions with the recipe object to make a prediction. 

```{r}
test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)
```
##Evaluating the model
###calculates common regression metrics such as RMSE, R-squared, and MAE between the observed and predicted values with metrics.
```{r}
metrics(test_data, truth = logQmean, estimate = lm_pred)
```

###model of the observed vs predicted
```{r}
ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +
  # Apply a gradient color scale
  scale_color_gradient2(low = "pink", mid = "purple", high = "navy") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() + 
  labs(title = "Linear Model: Observed vs Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Aridity")
```
###This really was a lot of work for one single fragile graph that can't test other algarithsms.

##Better approach: Workflow

```{r}
# Define model
lm_model <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
lm_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(lm_model) %>%
  # Fit the model to the training data
  fit(data = camels_train) 

# Extract the model coefficients from the workflow
summary(extract_fit_engine(lm_wf))$coefficients
```
```{r}
# From the base implementation
summary(lm_base)$coefficients
```
###Now workflow is embedded in the model!

##Make Predictions:

```{r}
#
lm_data <- augment(lm_wf, new_data = camels_test)
dim(lm_data)
```
```{r}
metrics(lm_data, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```
###Boom! an easy to make and more adaptble graph from my data!

```{r}
library(baguette)
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(rf_model) %>%
  # Fit the model
  fit(data = camels_train) 
```



```{r}
rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)
```
```{r}
metrics(rf_data, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```
### Here is the proof, using the framwork for a completely new model!

##Workflowset approach

```{r}
wf <- workflow_set(list(rec), list(lm_model, rf_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)
```
```{r}
rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```

#Question 3: Build a xgboost and neural network model using boost_tree
```{r}
xgb_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

```

##build a neural network model using the nnet engine from the baguette package using the bag_mlp function

```{r}
nnet_model <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")

```

##Add this to the above workflow
```{r}
wf_results <- workflow_set(
  preproc = list(camels_recipe = rec),
  models = list(
    lm_model = lm_model,
    rf_model = rf_model,
    xgb_model = xgb_model,
    nnet_model = nnet_model
  )
)

# Now apply fit_resamples and ASSIGN it
wf_results <- workflow_map(wf_results, "fit_resamples", resamples = camels_cv)

```



##Evaluate the model and compare it to the linear and random forest models

```{r}
collect_metrics(wf_results)
```

###Now I have the model results, I can run them through an evaluation to see what I will move forward with:

```{r}
rank_results(wf_results, rank_metric = "rsq", select_best = TRUE)
```


##Which of the 4 models would you move forward with?
###I will move forward with the neural network model as it performed best in cross-validation.

#Question 4a: Data Prep / Data Splitting
###Borrowing from the workflow presented above, build your own complete ML pipeline to predict mean streamflow using the CAMELS dataset. A successful model will have a R-squared value > 0.9.

```{r}
set.seed(13)
camels <- camels |> 
  mutate(logQmean = log(q_mean))
```


##Data Spliting
###Set a seed for reproducible
Create an initial split with 75% used for training and 25% for testing
Extract your training and testing sets
Build a 10-fold CV dataset as well

```{r}
camels_split_4 <- initial_split(camels, prop = 0.75)
camels_train_4 <- training(camels_split_4)
camels_test_4  <- testing(camels_split_4)

camels_cv_4 <- vfold_cv(camels_train_4, v = 10)
```


#Question 4b: Recipe 
###Define a formula you want to use to predict logQmean

##Describe in words why you are choosing the formula you are. Consult the downloaded PDF.

######I chose aridity and p_mean as predictors based the CAMELS showing that precipitation and aridity directly influence streamflow.

##for the data to help you make this decision.
Build a recipe that you feel handles the predictors chosen well

```{r}
rec_4 <- recipe(logQmean ~ aridity + p_mean, data = camels_train_4) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())
```


#Question 4c: Define 3 models 
###Define a random forest model using the rand_forest function
Set the engine to ranger and the mode to regression
Define two other models of your choice

```{r}
lm_model_4 <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

rf_model_4 <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

```
###I need a thrid model...
###I will move forward with recipe_nnet_model was the most accurate in Q3, so I will use it here.

```{r}
nnet_model_4 <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")

```


#Question 4d: workflow set 
###Create a workflow object
Add the recipe
Add the model(s)
Fit the model to the resamples

```{r}
wf_set_4 <- workflow_set(
  preproc = list(camels_recipe_4 = rec_4),  # named recipe
  models = list(
    lm_model   = lm_model_4,
    rf_model   = rf_model_4,
    nnet_model = nnet_model_4
  )
) %>%
  workflow_map("fit_resamples", resamples = camels_cv_4)

```

#Question 4e: Evaluation 
###Use autoplot and rank_results to compare the models.
Describe what model you think is best and why!

```{r}
rank_results(wf_set_4, rank_metric = "rsq", select_best = TRUE)
```


```{r}

autoplot(wf_set_4)

```
### As we can see, camels_recipe_4_nnet_model is the best model again in terms of rsq.

#Question 4f: Extract and Evaluate 
###Build a workflow (not workflow set) with your favorite model, recipe, and training data
Use fit to fit all training data to the model
Use augment to make predictions on the test data
Create a plot of the observed vs predicted values with clear title, axis labels, and a compelling color scale
Describe what you think of the results!

```{r}
final_rf_wf <- workflow() %>%
  add_recipe(rec_4) %>%
  add_model(rf_model_4) %>%
  fit(data = camels_train_4)

nnet_test_preds_4 <- augment(final_nnet_wf_4, new_data = camels_test_4)

rf_test_preds <- augment(final_rf_wf, new_data = camels_test_4)
metrics(rf_test_preds, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(nnet_test_preds_4, aes(x = logQmean, y = .pred, color = aridity)) +
  geom_point() +
  geom_abline(linetype = 2) +
  scale_color_viridis_c() +
  theme_linedraw() +
  labs(
    title = "Neural Net: Observed vs Predicted Log Mean Flow",
    x = "Observed Log Mean Flow",
    y = "Predicted Log Mean Flow",
    color = "Aridity")
```

###I think the results are too scattered ans heavily skewed so the data is not evenly spaced.  That said it appears to be more accurate than the other models used.
