[
  {
    "objectID": "Lab 6.html",
    "href": "Lab 6.html",
    "title": "Lab6",
    "section": "",
    "text": "#Question 1: Download data ##Set Up\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nWarning: package 'yardstick' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf', \n              mode = 'wb')\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\n# merge the data into a single data frame\ncamels &lt;- reduce(camels, power_full_join, by = 'gauge_id')\n\n\nlist.files(\"data\")\n\n[1] \"camels_attributes_v2.0.pdf\" \"camels_clim.txt\"           \n[3] \"camels_geol.txt\"            \"camels_hydro.txt\"          \n[5] \"camels_soil.txt\"            \"camels_topo.txt\"           \n[7] \"camels_vege.txt\"           \n\n\n###yay it worked! all 6 data files + the PDF are here\n#Question 2:Make 2 maps\n\np1 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\np1\n\n\n\n\n\n\n\n\n###It looks just like the map in the assignment!\n\n# First map: Color points by aridity\np2 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"pink\", high = \"navy\") +\n  labs(color = \"Aridity Index\") +\n  ggthemes::theme_map()\n\n# Second map: Color points by p_mean (mean annual precipitation)\np3 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"lightblue\", high =\"purple\") +\n  labs (color = \"p_mean (mm)\") +\n  ggthemes::theme_map()\n\n\np2 + p3\n\n\n\n\n\n\n\n\n##Model Preparation\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n###Looks like the expected results from the assignments.\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###The relationship is seen here just like in the assignment. There is a relationship between rainfall but it is not linear.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###Log scaled made a more leniar relationship, but clustared and unevenly distributed.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###Definitely more evenly spread after being skewed, just like expected.\n##Model building ###set seed for reproducabilty:\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n###use 80% of the data for training and 20% for testing with no stratification.\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n### 10-fold cross validation dataset to help us evaluate multi-model setups.\n\n###Separately we have used the recipe function to define a series of data preprocessing steps:\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n###fitting a linear model to the data.\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n###Just as expected\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n###Just as expected again\n##A Model’s ability to predict new data (incorrect tests) ###Using the wrong verion augment function to add predicted values to the test data won’t work because the recipe has not been applied\n\nnrow(camels_test)\n\n[1] 135\n\nnrow(camels_train)\n\n[1] 536\n\n\n###look how many more test vs train\n\n#broom::augment(lm_base, data = camels_test)\n\n###As expected, there is an erro, I will put it behind a “#” so that I can render this assignment.\n###Using the predict function to directly test the data without recipe onject we also see issues\n\ncamels_test$p2 = predict(lm_base, newdata = camels_test)\n\n## Scales way off!\nggplot(camels_test, aes(x = p2, y = logQmean)) + \n  geom_point() + \n  # Linear fit line, no error bands\n  geom_smooth(method = \"lm\", se = FALSE, size =1) +\n  # 1:1 line\n  geom_abline(color = \"red\", size = 1) + \n  labs(title = \"Linear Model Using `predict()`\",\n       x = \"Predicted Log Mean Flow\",\n       y = \"Observed Log Mean Flow\") + \n  theme_linedraw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###As expected.\n##Correct version ###prep -&gt; bake -&gt; predict; using the prep and bake functions with the recipe object to make a prediction.\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n##Evaluating the model ###calculates common regression metrics such as RMSE, R-squared, and MAE between the observed and predicted values with metrics.\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n###model of the observed vs predicted\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"pink\", mid = \"purple\", high = \"navy\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n###This really was a lot of work for one single fragile graph that can’t test other algarithsms.\n##Better approach: Workflow\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n###Now workflow is embedded in the model!\n##Make Predictions:\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  62\n\n\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n###Boom! an easy to make and more adaptble graph from my data!\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  61\n\n\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.596\n2 rsq     standard       0.733\n3 mae     standard       0.370\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nHere is the proof, using the framwork for a completely new model!\n##Workflowset approach\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.565  0.0243    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0255    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\n#Question 3: Build a xgboost and neural network model using boost_tree\n\nxgb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n##build a neural network model using the nnet engine from the baguette package using the bag_mlp function\n\nnnet_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n##Add this to the above workflow\n\nwf_results &lt;- workflow_set(\n  preproc = list(camels_recipe = rec),\n  models = list(\n    lm_model = lm_model,\n    rf_model = rf_model,\n    xgb_model = xgb_model,\n    nnet_model = nnet_model\n  )\n)\n\n# Now apply fit_resamples and ASSIGN it\nwf_results &lt;- workflow_map(wf_results, \"fit_resamples\", resamples = camels_cv)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n##Evaluate the model and compare it to the linear and random forest models\n\ncollect_metrics(wf_results)\n\n# A tibble: 8 × 9\n  wflow_id          .config preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 camels_recipe_lm… Prepro… recipe  line… rmse    standard   0.569    10  0.0260\n2 camels_recipe_lm… Prepro… recipe  line… rsq     standard   0.770    10  0.0223\n3 camels_recipe_rf… Prepro… recipe  rand… rmse    standard   0.565    10  0.0253\n4 camels_recipe_rf… Prepro… recipe  rand… rsq     standard   0.770    10  0.0264\n5 camels_recipe_xg… Prepro… recipe  boos… rmse    standard   0.600    10  0.0289\n6 camels_recipe_xg… Prepro… recipe  boos… rsq     standard   0.745    10  0.0268\n7 camels_recipe_nn… Prepro… recipe  bag_… rmse    standard   0.547    10  0.0309\n8 camels_recipe_nn… Prepro… recipe  bag_… rsq     standard   0.787    10  0.0267\n\n\n###Now I have the model results, I can run them through an evaluation to see what I will move forward with:\n\nrank_results(wf_results, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 camels_recipe_nn… Prepro… rmse    0.547  0.0309    10 recipe       bag_…     1\n2 camels_recipe_nn… Prepro… rsq     0.787  0.0267    10 recipe       bag_…     1\n3 camels_recipe_rf… Prepro… rmse    0.565  0.0253    10 recipe       rand…     2\n4 camels_recipe_rf… Prepro… rsq     0.770  0.0264    10 recipe       rand…     2\n5 camels_recipe_lm… Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 camels_recipe_lm… Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 camels_recipe_xg… Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 camels_recipe_xg… Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\n\n##Which of the 4 models would you move forward with? ###I will move forward with the neural network model as it performed best in cross-validation.\n#Question 4a: Data Prep / Data Splitting ###Borrowing from the workflow presented above, build your own complete ML pipeline to predict mean streamflow using the CAMELS dataset. A successful model will have a R-squared value &gt; 0.9.\n\nset.seed(13)\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n##Data Spliting ###Set a seed for reproducible Create an initial split with 75% used for training and 25% for testing Extract your training and testing sets Build a 10-fold CV dataset as well\n\ncamels_split_4 &lt;- initial_split(camels, prop = 0.75)\ncamels_train_4 &lt;- training(camels_split_4)\ncamels_test_4  &lt;- testing(camels_split_4)\n\ncamels_cv_4 &lt;- vfold_cv(camels_train_4, v = 10)\n\n#Question 4b: Recipe ###Define a formula you want to use to predict logQmean\n##Describe in words why you are choosing the formula you are. Consult the downloaded PDF.\n######I chose aridity and p_mean as predictors based the CAMELS showing that precipitation and aridity directly influence streamflow.\n##for the data to help you make this decision. Build a recipe that you feel handles the predictors chosen well\n\nrec_4 &lt;- recipe(logQmean ~ aridity + p_mean, data = camels_train_4) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n#Question 4c: Define 3 models ###Define a random forest model using the rand_forest function Set the engine to ranger and the mode to regression Define two other models of your choice\n\nlm_model_4 &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\nrf_model_4 &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n###I need a thrid model… ###I will move forward with recipe_nnet_model was the most accurate in Q3, so I will use it here.\n\nnnet_model_4 &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n#Question 4d: workflow set ###Create a workflow object Add the recipe Add the model(s) Fit the model to the resamples\n\nwf_set_4 &lt;- workflow_set(\n  preproc = list(camels_recipe_4 = rec_4),  # named recipe\n  models = list(\n    lm_model   = lm_model_4,\n    rf_model   = rf_model_4,\n    nnet_model = nnet_model_4\n  )\n) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv_4)\n\n#Question 4e: Evaluation ###Use autoplot and rank_results to compare the models. Describe what model you think is best and why!\n\nrank_results(wf_set_4, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 camels_recipe_4_… Prepro… rmse    0.543  0.0304    10 recipe       bag_…     1\n2 camels_recipe_4_… Prepro… rsq     0.783  0.0252    10 recipe       bag_…     1\n3 camels_recipe_4_… Prepro… rmse    0.551  0.0323    10 recipe       rand…     2\n4 camels_recipe_4_… Prepro… rsq     0.772  0.0280    10 recipe       rand…     2\n5 camels_recipe_4_… Prepro… rmse    0.570  0.0339    10 recipe       line…     3\n6 camels_recipe_4_… Prepro… rsq     0.762  0.0253    10 recipe       line…     3\n\n\n\nautoplot(wf_set_4)\n\n\n\n\n\n\n\n\n\n\nAs we can see, camels_recipe_4_nnet_model is the best model again in terms of rsq.\n#Question 4f: Extract and Evaluate ###Build a workflow (not workflow set) with your favorite model, recipe, and training data Use fit to fit all training data to the model Use augment to make predictions on the test data Create a plot of the observed vs predicted values with clear title, axis labels, and a compelling color scale Describe what you think of the results!\n\nfinal_nnet_wf_4 &lt;- extract_workflow(wf_set_4, \"camels_recipe_4_nnet_model\")\nfinal_nnet_wf_4 &lt;- final_nnet_wf_4 %&gt;%\n  fit(data = camels_train_4)\n\nnnet_test_preds_4 &lt;- augment(final_nnet_wf_4, new_data = camels_test_4)\n\n# View metrics\nmetrics(nnet_test_preds_4, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.616\n2 rsq     standard       0.743\n3 mae     standard       0.360\n\n\n\nggplot(nnet_test_preds_4, aes(x = logQmean, y = .pred, color = aridity)) +\n  geom_point(alpha = 0.7) +\n  geom_abline(linetype = \"dashed\", color = \"red\") +\n  scale_color_viridis_c(trans = \"log\") +\n  theme_linedraw() +\n  labs(\n    title = \"Neural Net: Observed vs Predicted Log Mean Flow\",\n    x = \"Observed Log Mean Flow\",\n    y = \"Predicted Log Mean Flow\",\n    color = \"Aridity (log scale)\"\n  )\n\n\n\n\n\n\n\n\n###I think the results are slighlty scattered but far ess and more evenly spaced than the other prediction models graphed."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lab6",
    "section": "",
    "text": "#Question 1: Download data ##Set Up\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nWarning: package 'yardstick' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf', \n              mode = 'wb')\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\n# merge the data into a single data frame\ncamels &lt;- reduce(camels, power_full_join, by = 'gauge_id')\n\n\nlist.files(\"data\")\n\n[1] \"camels_attributes_v2.0.pdf\" \"camels_clim.txt\"           \n[3] \"camels_geol.txt\"            \"camels_hydro.txt\"          \n[5] \"camels_soil.txt\"            \"camels_topo.txt\"           \n[7] \"camels_vege.txt\"           \n\n\n###yay it worked! all 6 data files + the PDF are here\n#Question 2:Make 2 maps\n\np1 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\np1\n\n\n\n\n\n\n\n\n###It looks just like the map in the assignment!\n\n# First map: Color points by aridity\np2 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"pink\", high = \"navy\") +\n  labs(color = \"Aridity Index\") +\n  ggthemes::theme_map()\n\n# Second map: Color points by p_mean (mean annual precipitation)\np3 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"lightblue\", high =\"purple\") +\n  labs (color = \"p_mean (mm)\") +\n  ggthemes::theme_map()\n\n\np2 + p3\n\n\n\n\n\n\n\n\n##Model Preparation\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n###Looks like the expected results from the assignments.\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###The relationship is seen here just like in the assignment. There is a relationship between rainfall but it is not linear.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###Log scaled made a more leniar relationship, but clustared and unevenly distributed.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###Definitely more evenly spread after being skewed, just like expected.\n##Model building ###set seed for reproducabilty:\n\n\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n###use 80% of the data for training and 20% for testing with no stratification.\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n### 10-fold cross validation dataset to help us evaluate multi-model setups.\n\n\n\n###Separately we have used the recipe function to define a series of data preprocessing steps:\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n###fitting a linear model to the data.\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n###Just as expected\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n###Just as expected again\n##A Model’s ability to predict new data (incorrect tests) ###Using the wrong verion augment function to add predicted values to the test data won’t work because the recipe has not been applied\n\nnrow(camels_test)\n\n[1] 135\n\nnrow(camels_train)\n\n[1] 536\n\n\n###look how many more test vs train\n\n#broom::augment(lm_base, data = camels_test)\n\n###As expected, there is an erro, I will put it behind a “#” so that I can render this assignment.\n###Using the predict function to directly test the data without recipe onject we also see issues\n\ncamels_test$p2 = predict(lm_base, newdata = camels_test)\n\n## Scales way off!\nggplot(camels_test, aes(x = p2, y = logQmean)) + \n  geom_point() + \n  # Linear fit line, no error bands\n  geom_smooth(method = \"lm\", se = FALSE, size =1) +\n  # 1:1 line\n  geom_abline(color = \"red\", size = 1) + \n  labs(title = \"Linear Model Using `predict()`\",\n       x = \"Predicted Log Mean Flow\",\n       y = \"Observed Log Mean Flow\") + \n  theme_linedraw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###As expected.\n##Correct version ###prep -&gt; bake -&gt; predict; using the prep and bake functions with the recipe object to make a prediction.\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n##Evaluating the model ###calculates common regression metrics such as RMSE, R-squared, and MAE between the observed and predicted values with metrics.\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n###model of the observed vs predicted\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"pink\", mid = \"purple\", high = \"navy\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n###This really was a lot of work for one single fragile graph that can’t test other algarithsms.\n##Better approach: Workflow\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n###Now workflow is embedded in the model!\n##Make Predictions:\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  62\n\n\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n###Boom! an easy to make and more adaptble graph from my data!\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  61\n\n\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.596\n2 rsq     standard       0.733\n3 mae     standard       0.370\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nHere is the proof, using the framwork for a completely new model!\n##Workflowset approach\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.565  0.0243    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0255    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\n#Question 3: Build a xgboost and neural network model using boost_tree\n\nxgb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n##build a neural network model using the nnet engine from the baguette package using the bag_mlp function\n\nnnet_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n##Add this to the above workflow\n\nwf_results &lt;- workflow_set(\n  preproc = list(camels_recipe = rec),\n  models = list(\n    lm_model = lm_model,\n    rf_model = rf_model,\n    xgb_model = xgb_model,\n    nnet_model = nnet_model\n  )\n)\n\n# Now apply fit_resamples and ASSIGN it\nwf_results &lt;- workflow_map(wf_results, \"fit_resamples\", resamples = camels_cv)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n##Evaluate the model and compare it to the linear and random forest models\n\ncollect_metrics(wf_results)\n\n# A tibble: 8 × 9\n  wflow_id          .config preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 camels_recipe_lm… Prepro… recipe  line… rmse    standard   0.569    10  0.0260\n2 camels_recipe_lm… Prepro… recipe  line… rsq     standard   0.770    10  0.0223\n3 camels_recipe_rf… Prepro… recipe  rand… rmse    standard   0.565    10  0.0253\n4 camels_recipe_rf… Prepro… recipe  rand… rsq     standard   0.770    10  0.0264\n5 camels_recipe_xg… Prepro… recipe  boos… rmse    standard   0.600    10  0.0289\n6 camels_recipe_xg… Prepro… recipe  boos… rsq     standard   0.745    10  0.0268\n7 camels_recipe_nn… Prepro… recipe  bag_… rmse    standard   0.547    10  0.0309\n8 camels_recipe_nn… Prepro… recipe  bag_… rsq     standard   0.787    10  0.0267\n\n\n###Now I have the model results, I can run them through an evaluation to see what I will move forward with:\n\nrank_results(wf_results, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 camels_recipe_nn… Prepro… rmse    0.547  0.0309    10 recipe       bag_…     1\n2 camels_recipe_nn… Prepro… rsq     0.787  0.0267    10 recipe       bag_…     1\n3 camels_recipe_rf… Prepro… rmse    0.565  0.0253    10 recipe       rand…     2\n4 camels_recipe_rf… Prepro… rsq     0.770  0.0264    10 recipe       rand…     2\n5 camels_recipe_lm… Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 camels_recipe_lm… Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 camels_recipe_xg… Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 camels_recipe_xg… Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\n\n##Which of the 4 models would you move forward with? ###I will move forward with the neural network model as it performed best in cross-validation.\n#Question 4a: Data Prep / Data Splitting ###Borrowing from the workflow presented above, build your own complete ML pipeline to predict mean streamflow using the CAMELS dataset. A successful model will have a R-squared value &gt; 0.9.\n\nset.seed(13)\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n##Data Spliting ###Set a seed for reproducible Create an initial split with 75% used for training and 25% for testing Extract your training and testing sets Build a 10-fold CV dataset as well\n\ncamels_split_4 &lt;- initial_split(camels, prop = 0.75)\ncamels_train_4 &lt;- training(camels_split_4)\ncamels_test_4  &lt;- testing(camels_split_4)\n\ncamels_cv_4 &lt;- vfold_cv(camels_train_4, v = 10)\n\n#Question 4b: Recipe ###Define a formula you want to use to predict logQmean\n##Describe in words why you are choosing the formula you are. Consult the downloaded PDF.\n######I chose aridity and p_mean as predictors based the CAMELS showing that precipitation and aridity directly influence streamflow.\n##for the data to help you make this decision. Build a recipe that you feel handles the predictors chosen well\n\nrec_4 &lt;- recipe(logQmean ~ aridity + p_mean, data = camels_train_4) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n#Question 4c: Define 3 models ###Define a random forest model using the rand_forest function Set the engine to ranger and the mode to regression Define two other models of your choice\n\nlm_model_4 &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\nrf_model_4 &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n###I need a thrid model… ###I will move forward with recipe_nnet_model was the most accurate in Q3, so I will use it here.\n\nnnet_model_4 &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n#Question 4d: workflow set ###Create a workflow object Add the recipe Add the model(s) Fit the model to the resamples\n\nwf_set_4 &lt;- workflow_set(\n  preproc = list(camels_recipe_4 = rec_4),  # named recipe\n  models = list(\n    lm_model   = lm_model_4,\n    rf_model   = rf_model_4,\n    nnet_model = nnet_model_4\n  )\n) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv_4)\n\n#Question 4e: Evaluation ###Use autoplot and rank_results to compare the models. Describe what model you think is best and why!\n\nrank_results(wf_set_4, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 camels_recipe_4_… Prepro… rmse    0.543  0.0304    10 recipe       bag_…     1\n2 camels_recipe_4_… Prepro… rsq     0.783  0.0252    10 recipe       bag_…     1\n3 camels_recipe_4_… Prepro… rmse    0.551  0.0323    10 recipe       rand…     2\n4 camels_recipe_4_… Prepro… rsq     0.772  0.0280    10 recipe       rand…     2\n5 camels_recipe_4_… Prepro… rmse    0.570  0.0339    10 recipe       line…     3\n6 camels_recipe_4_… Prepro… rsq     0.762  0.0253    10 recipe       line…     3\n\n\n\nautoplot(wf_set_4)\n\n\n\n\n\n\n\n\n\n\nAs we can see, camels_recipe_4_nnet_model is the best model again in terms of rsq.\n#Question 4f: Extract and Evaluate ###Build a workflow (not workflow set) with your favorite model, recipe, and training data Use fit to fit all training data to the model Use augment to make predictions on the test data Create a plot of the observed vs predicted values with clear title, axis labels, and a compelling color scale Describe what you think of the results!\n\nfinal_nnet_wf_4 &lt;- extract_workflow(wf_set_4, \"camels_recipe_4_nnet_model\")\nfinal_nnet_wf_4 &lt;- final_nnet_wf_4 %&gt;%\n  fit(data = camels_train_4)\n\nnnet_test_preds_4 &lt;- augment(final_nnet_wf_4, new_data = camels_test_4)\n\n# View metrics\nmetrics(nnet_test_preds_4, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.616\n2 rsq     standard       0.743\n3 mae     standard       0.360\n\n\n\nggplot(nnet_test_preds_4, aes(x = logQmean, y = .pred, color = aridity)) +\n  geom_point(alpha = 0.7) +\n  geom_abline(linetype = \"dashed\", color = \"red\") +\n  scale_color_viridis_c(trans = \"log\") +\n  theme_linedraw() +\n  labs(\n    title = \"Neural Net: Observed vs Predicted Log Mean Flow\",\n    x = \"Observed Log Mean Flow\",\n    y = \"Predicted Log Mean Flow\",\n    color = \"Aridity (log scale)\"\n  )\n\n\n\n\n\n\n\n\n###I think the results are slighlty scattered but far ess and more evenly spaced than the other prediction models graphed."
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "hyperparameter-tuning",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nWarning: package 'yardstick' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.4.3\n\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\nlibrary(rsample)\nlibrary(recipes)\nlibrary(dplyr)\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n\n#Read in the data using map, read_delim() and powerjoin::power_full_join(). If you need a refresher on this please refer to lab 6.\nlocal_files &lt;- list.files(\n  path = \"data\",      \n  pattern = \"\\\\.txt$\",      \n  full.names = TRUE\n)\n\n# 2. Read all files into a list of data frames\ncamels_list &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\n# 3. Merge them on gauge_id using power_full_join\ncamels &lt;- reduce(camels_list, power_full_join, by = \"gauge_id\")\n\n# 4. Preview the result to confirm it's loaded\nglimpse(camels)\n\nRows: 671\nColumns: 58\n$ gauge_id             &lt;chr&gt; \"01013500\", \"01022500\", \"01030500\", \"01031500\", \"…\n$ p_mean               &lt;dbl&gt; 3.126679, 3.608126, 3.274405, 3.522957, 3.323146,…\n$ pet_mean             &lt;dbl&gt; 1.971555, 2.119256, 2.043594, 2.071324, 2.090024,…\n$ p_seasonality        &lt;dbl&gt; 0.187940259, -0.114529586, 0.047358189, 0.1040905…\n$ frac_snow            &lt;dbl&gt; 0.3134404, 0.2452590, 0.2770184, 0.2918365, 0.280…\n$ aridity              &lt;dbl&gt; 0.6305587, 0.5873564, 0.6241114, 0.5879503, 0.628…\n$ high_prec_freq       &lt;dbl&gt; 12.95, 20.55, 17.15, 18.90, 20.10, 13.50, 17.50, …\n$ high_prec_dur        &lt;dbl&gt; 1.348958, 1.205279, 1.207746, 1.148936, 1.165217,…\n$ high_prec_timing     &lt;chr&gt; \"son\", \"son\", \"son\", \"son\", \"son\", \"jja\", \"son\", …\n$ low_prec_freq        &lt;dbl&gt; 202.20, 233.65, 215.60, 227.35, 235.90, 193.50, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.427119, 3.662226, 3.514262, 3.473644, 3.691706,…\n$ low_prec_timing      &lt;chr&gt; \"mam\", \"jja\", \"djf\", \"djf\", \"djf\", \"mam\", \"mam\", …\n$ geol_1st_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Acid plutonic…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.8159044, 0.5906582, 0.5733054, 0.4489279, 0.308…\n$ geol_2nd_class       &lt;chr&gt; \"Basic volcanic rocks\", \"Siliciclastic sedimentar…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.17972945, 0.16461821, 0.28701001, 0.44386282, 0…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.000000000, 0.000000000, 0.052140094, 0.02625797…\n$ geol_porostiy        &lt;dbl&gt; 0.1714, 0.0710, 0.1178, 0.0747, 0.0522, 0.0711, 0…\n$ geol_permeability    &lt;dbl&gt; -14.7019, -14.2138, -14.4918, -14.8410, -14.4819,…\n$ q_mean               &lt;dbl&gt; 1.699155, 2.173062, 1.820108, 2.030242, 2.182870,…\n$ runoff_ratio         &lt;dbl&gt; 0.5434375, 0.6022689, 0.5558590, 0.5762893, 0.656…\n$ slope_fdc            &lt;dbl&gt; 1.528219, 1.776280, 1.871110, 1.494019, 1.415939,…\n$ baseflow_index       &lt;dbl&gt; 0.5852260, 0.5544784, 0.5084407, 0.4450905, 0.473…\n$ stream_elas          &lt;dbl&gt; 1.8453242, 1.7027824, 1.3775052, 1.6486930, 1.510…\n$ q5                   &lt;dbl&gt; 0.24110613, 0.20473436, 0.10714920, 0.11134535, 0…\n$ q95                  &lt;dbl&gt; 6.373021, 7.123049, 6.854887, 8.010503, 8.095148,…\n$ high_q_freq          &lt;dbl&gt; 6.10, 3.90, 12.25, 18.90, 14.95, 14.10, 16.05, 16…\n$ high_q_dur           &lt;dbl&gt; 8.714286, 2.294118, 7.205882, 3.286957, 2.577586,…\n$ low_q_freq           &lt;dbl&gt; 41.35, 65.15, 89.25, 94.80, 71.55, 58.90, 82.20, …\n$ low_q_dur            &lt;dbl&gt; 20.170732, 17.144737, 19.402174, 14.697674, 12.77…\n$ zero_q_freq          &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0…\n$ hfd_mean             &lt;dbl&gt; 207.25, 166.25, 184.90, 181.00, 184.80, 197.20, 1…\n$ soil_depth_pelletier &lt;dbl&gt; 7.4047619, 17.4128079, 19.0114144, 7.2525570, 5.3…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.248408, 1.491846, 1.461363, 1.279047, 1.392779,…\n$ soil_porosity        &lt;dbl&gt; 0.4611488, 0.4159055, 0.4590910, 0.4502360, 0.422…\n$ soil_conductivity    &lt;dbl&gt; 1.106522, 2.375005, 1.289807, 1.373292, 2.615154,…\n$ max_water_content    &lt;dbl&gt; 0.5580548, 0.6262289, 0.6530198, 0.5591227, 0.561…\n$ sand_frac            &lt;dbl&gt; 27.84183, 59.39016, 32.23546, 35.26903, 55.16313,…\n$ silt_frac            &lt;dbl&gt; 55.15694, 28.08094, 51.77918, 50.84123, 34.18544,…\n$ clay_frac            &lt;dbl&gt; 16.275732, 12.037646, 14.776824, 12.654125, 10.30…\n$ water_frac           &lt;dbl&gt; 5.3766978, 1.2269127, 1.6343449, 0.6745936, 0.000…\n$ organic_frac         &lt;dbl&gt; 0.4087168, 0.0000000, 1.3302776, 0.0000000, 0.000…\n$ other_frac           &lt;dbl&gt; 0.0000000, 0.3584723, 0.0220161, 0.0000000, 0.147…\n$ gauge_lat            &lt;dbl&gt; 47.23739, 44.60797, 45.50097, 45.17501, 44.86920,…\n$ gauge_lon            &lt;dbl&gt; -68.58264, -67.93524, -68.30596, -69.31470, -69.9…\n$ elev_mean            &lt;dbl&gt; 250.31, 92.68, 143.80, 247.80, 310.38, 615.70, 47…\n$ slope_mean           &lt;dbl&gt; 21.64152, 17.79072, 12.79195, 29.56035, 49.92122,…\n$ area_gages2          &lt;dbl&gt; 2252.70, 573.60, 3676.17, 769.05, 909.10, 383.82,…\n$ area_geospa_fabric   &lt;dbl&gt; 2303.95, 620.38, 3676.09, 766.53, 904.94, 396.10,…\n$ frac_forest          &lt;dbl&gt; 0.9063, 0.9232, 0.8782, 0.9548, 0.9906, 1.0000, 1…\n$ lai_max              &lt;dbl&gt; 4.167304, 4.871392, 4.685200, 4.903259, 5.086811,…\n$ lai_diff             &lt;dbl&gt; 3.340732, 3.746692, 3.665543, 3.990843, 4.300978,…\n$ gvf_max              &lt;dbl&gt; 0.8045674, 0.8639358, 0.8585020, 0.8706685, 0.891…\n$ gvf_diff             &lt;dbl&gt; 0.3716482, 0.3377125, 0.3513934, 0.3986194, 0.445…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.8834519, 0.8204934, 0.9752580, 1.0000000, 0.850…\n$ dom_land_cover       &lt;chr&gt; \"    Mixed Forests\", \"    Mixed Forests\", \"    Mi…\n$ root_depth_50        &lt;dbl&gt; NA, 0.2374345, NA, 0.2500000, 0.2410270, 0.225615…\n$ root_depth_99        &lt;dbl&gt; NA, 2.238444, NA, 2.400000, 2.340180, 2.237435, 2…\n\n#Clean the data using dplyr, EDA (skimr, visdat, ggpubr), and other means to ensure it is in a good form for modeling.\nskimr::skim(camels)\n\n\nData summary\n\n\nName\ncamels\n\n\nNumber of rows\n671\n\n\nNumber of columns\n58\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n52\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngauge_id\n0\n1.00\n8\n8\n0\n671\n0\n\n\nhigh_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\nlow_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\ngeol_1st_class\n0\n1.00\n12\n31\n0\n12\n0\n\n\ngeol_2nd_class\n138\n0.79\n12\n31\n0\n13\n0\n\n\ndom_land_cover\n0\n1.00\n12\n38\n0\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\np_mean\n0\n1.00\n3.26\n1.41\n0.64\n2.37\n3.23\n3.78\n8.94\n▃▇▂▁▁\n\n\npet_mean\n0\n1.00\n2.79\n0.55\n1.90\n2.34\n2.69\n3.15\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1.00\n-0.04\n0.53\n-1.44\n-0.26\n0.08\n0.22\n0.92\n▁▂▃▇▂\n\n\nfrac_snow\n0\n1.00\n0.18\n0.20\n0.00\n0.04\n0.10\n0.22\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1.00\n1.06\n0.62\n0.22\n0.70\n0.86\n1.27\n5.21\n▇▂▁▁▁\n\n\nhigh_prec_freq\n0\n1.00\n20.93\n4.55\n7.90\n18.50\n22.00\n24.23\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1.00\n1.35\n0.19\n1.08\n1.21\n1.28\n1.44\n2.09\n▇▅▂▁▁\n\n\nlow_prec_freq\n0\n1.00\n254.65\n35.12\n169.90\n232.70\n255.85\n278.92\n348.70\n▂▅▇▅▁\n\n\nlow_prec_dur\n0\n1.00\n5.95\n3.20\n2.79\n4.24\n4.95\n6.70\n36.51\n▇▁▁▁▁\n\n\nglim_1st_class_frac\n0\n1.00\n0.79\n0.20\n0.30\n0.61\n0.83\n1.00\n1.00\n▁▃▃▃▇\n\n\nglim_2nd_class_frac\n0\n1.00\n0.16\n0.14\n0.00\n0.00\n0.14\n0.27\n0.49\n▇▃▃▂▁\n\n\ncarbonate_rocks_frac\n0\n1.00\n0.12\n0.26\n0.00\n0.00\n0.00\n0.04\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n3\n1.00\n0.13\n0.07\n0.01\n0.07\n0.13\n0.19\n0.28\n▇▆▇▇▂\n\n\ngeol_permeability\n0\n1.00\n-13.89\n1.18\n-16.50\n-14.77\n-13.96\n-13.00\n-10.90\n▂▅▇▅▂\n\n\nq_mean\n1\n1.00\n1.49\n1.54\n0.00\n0.63\n1.13\n1.75\n9.69\n▇▁▁▁▁\n\n\nrunoff_ratio\n1\n1.00\n0.39\n0.23\n0.00\n0.24\n0.35\n0.51\n1.36\n▆▇▂▁▁\n\n\nslope_fdc\n1\n1.00\n1.24\n0.51\n0.00\n0.90\n1.28\n1.63\n2.50\n▂▅▇▇▁\n\n\nbaseflow_index\n0\n1.00\n0.49\n0.16\n0.01\n0.40\n0.50\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n1\n1.00\n1.83\n0.78\n-0.64\n1.32\n1.70\n2.23\n6.24\n▁▇▃▁▁\n\n\nq5\n1\n1.00\n0.17\n0.27\n0.00\n0.01\n0.08\n0.22\n2.42\n▇▁▁▁▁\n\n\nq95\n1\n1.00\n5.06\n4.94\n0.00\n2.07\n3.77\n6.29\n31.82\n▇▂▁▁▁\n\n\nhigh_q_freq\n1\n1.00\n25.74\n29.07\n0.00\n6.41\n15.10\n35.79\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n1\n1.00\n6.91\n10.07\n0.00\n1.82\n2.85\n7.55\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n1\n1.00\n107.62\n82.24\n0.00\n37.44\n96.00\n162.14\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n1\n1.00\n22.28\n21.66\n0.00\n10.00\n15.52\n26.91\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n1\n1.00\n0.03\n0.11\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n1\n1.00\n182.52\n33.53\n112.25\n160.16\n173.77\n204.05\n287.75\n▂▇▃▂▁\n\n\nsoil_depth_pelletier\n0\n1.00\n10.87\n16.24\n0.27\n1.00\n1.23\n12.89\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1.00\n1.29\n0.27\n0.40\n1.11\n1.46\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1.00\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.68\n▃▇▁▁▁\n\n\nsoil_conductivity\n0\n1.00\n1.74\n1.52\n0.45\n0.93\n1.35\n1.93\n13.96\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1.00\n0.53\n0.15\n0.09\n0.43\n0.56\n0.64\n1.05\n▁▅▇▃▁\n\n\nsand_frac\n0\n1.00\n36.47\n15.63\n8.18\n25.44\n35.27\n44.46\n91.98\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1.00\n33.86\n13.25\n2.99\n23.95\n34.06\n43.64\n67.77\n▂▆▇▆▁\n\n\nclay_frac\n0\n1.00\n19.89\n9.32\n1.85\n14.00\n18.66\n25.42\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1.00\n0.10\n0.94\n0.00\n0.00\n0.00\n0.00\n19.35\n▇▁▁▁▁\n\n\norganic_frac\n0\n1.00\n0.59\n3.84\n0.00\n0.00\n0.00\n0.00\n57.86\n▇▁▁▁▁\n\n\nother_frac\n0\n1.00\n9.82\n16.83\n0.00\n0.00\n1.31\n11.74\n99.38\n▇▁▁▁▁\n\n\ngauge_lat\n0\n1.00\n39.24\n5.21\n27.05\n35.70\n39.25\n43.21\n48.82\n▂▃▇▆▅\n\n\ngauge_lon\n0\n1.00\n-95.79\n16.21\n-124.39\n-110.41\n-92.78\n-81.77\n-67.94\n▆▃▇▇▅\n\n\nelev_mean\n0\n1.00\n759.42\n786.00\n10.21\n249.67\n462.72\n928.88\n3571.18\n▇▂▁▁▁\n\n\nslope_mean\n0\n1.00\n46.20\n47.12\n0.82\n7.43\n28.80\n73.17\n255.69\n▇▂▂▁▁\n\n\narea_gages2\n0\n1.00\n792.62\n1701.95\n4.03\n122.28\n329.68\n794.30\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1.00\n808.08\n1709.85\n4.10\n127.98\n340.70\n804.50\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1.00\n0.64\n0.37\n0.00\n0.28\n0.81\n0.97\n1.00\n▃▁▁▂▇\n\n\nlai_max\n0\n1.00\n3.22\n1.52\n0.37\n1.81\n3.37\n4.70\n5.58\n▅▆▃▅▇\n\n\nlai_diff\n0\n1.00\n2.45\n1.33\n0.15\n1.20\n2.34\n3.76\n4.83\n▇▇▇▆▇\n\n\ngvf_max\n0\n1.00\n0.72\n0.17\n0.18\n0.61\n0.78\n0.86\n0.92\n▁▁▂▃▇\n\n\ngvf_diff\n0\n1.00\n0.32\n0.15\n0.03\n0.19\n0.32\n0.46\n0.65\n▃▇▅▇▁\n\n\ndom_land_cover_frac\n0\n1.00\n0.81\n0.18\n0.31\n0.65\n0.86\n1.00\n1.00\n▁▂▃▃▇\n\n\nroot_depth_50\n24\n0.96\n0.18\n0.03\n0.12\n0.17\n0.18\n0.19\n0.25\n▃▃▇▂▂\n\n\nroot_depth_99\n24\n0.96\n1.83\n0.30\n1.50\n1.52\n1.80\n2.00\n3.10\n▇▃▂▁▁\n\n\n\n\nvisdat::vis_miss(camels)\n\n\n\n\n\n\n\n\nalright so the data needs only a bit of cleaning, only 0.5% of the data is missing! Most of that seems to be from geol_2nd_class. Time to clean…\n\ncamels_clean &lt;- camels %&gt;%\n  filter(!is.na(q_mean), !is.na(geol_2nd_class)) %&gt;%\n  drop_na()\n\nskimr::skim(camels_clean)\n\n\nData summary\n\n\nName\ncamels_clean\n\n\nNumber of rows\n507\n\n\nNumber of columns\n58\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n52\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngauge_id\n0\n1\n8\n8\n0\n507\n0\n\n\nhigh_prec_timing\n0\n1\n3\n3\n0\n4\n0\n\n\nlow_prec_timing\n0\n1\n3\n3\n0\n4\n0\n\n\ngeol_1st_class\n0\n1\n12\n31\n0\n12\n0\n\n\ngeol_2nd_class\n0\n1\n12\n31\n0\n13\n0\n\n\ndom_land_cover\n0\n1\n12\n38\n0\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\np_mean\n0\n1\n3.15\n1.49\n0.64\n2.20\n3.05\n3.70\n8.94\n▅▇▂▁▁\n\n\npet_mean\n0\n1\n2.80\n0.56\n1.94\n2.37\n2.70\n3.16\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1\n-0.06\n0.56\n-1.44\n-0.41\n0.08\n0.27\n0.92\n▂▃▃▇▃\n\n\nfrac_snow\n0\n1\n0.19\n0.21\n0.00\n0.04\n0.11\n0.26\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1\n1.13\n0.67\n0.22\n0.73\n0.89\n1.45\n5.21\n▇▃▁▁▁\n\n\nhigh_prec_freq\n0\n1\n21.00\n4.80\n7.90\n18.45\n22.20\n24.55\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1\n1.37\n0.19\n1.10\n1.22\n1.32\n1.47\n2.07\n▇▅▂▁▁\n\n\nlow_prec_freq\n0\n1\n256.92\n36.76\n169.90\n233.57\n259.15\n286.75\n348.70\n▂▅▇▆▁\n\n\nlow_prec_dur\n0\n1\n6.29\n3.46\n2.79\n4.42\n5.14\n7.21\n36.51\n▇▁▁▁▁\n\n\nglim_1st_class_frac\n0\n1\n0.73\n0.19\n0.30\n0.58\n0.76\n0.90\n1.00\n▂▅▅▆▇\n\n\nglim_2nd_class_frac\n0\n1\n0.19\n0.13\n0.00\n0.08\n0.19\n0.30\n0.49\n▇▆▆▅▂\n\n\ncarbonate_rocks_frac\n0\n1\n0.13\n0.26\n0.00\n0.00\n0.00\n0.09\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n0\n1\n0.12\n0.06\n0.01\n0.07\n0.12\n0.17\n0.28\n▆▆▇▆▁\n\n\ngeol_permeability\n0\n1\n-13.86\n1.13\n-16.50\n-14.66\n-13.90\n-13.03\n-10.97\n▂▃▇▅▁\n\n\nq_mean\n0\n1\n1.46\n1.61\n0.00\n0.49\n1.00\n1.72\n9.50\n▇▁▁▁▁\n\n\nrunoff_ratio\n0\n1\n0.38\n0.24\n0.00\n0.22\n0.34\n0.51\n1.36\n▇▇▃▁▁\n\n\nslope_fdc\n0\n1\n1.19\n0.53\n0.00\n0.81\n1.24\n1.56\n2.50\n▃▆▇▆▁\n\n\nbaseflow_index\n0\n1\n0.49\n0.17\n0.01\n0.39\n0.51\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n0\n1\n1.86\n0.80\n-0.64\n1.33\n1.69\n2.25\n6.24\n▁▇▃▁▁\n\n\nq5\n0\n1\n0.17\n0.25\n0.00\n0.01\n0.08\n0.22\n1.77\n▇▁▁▁▁\n\n\nq95\n0\n1\n4.98\n5.23\n0.00\n1.67\n3.46\n6.29\n31.23\n▇▂▁▁▁\n\n\nhigh_q_freq\n0\n1\n27.15\n30.51\n0.00\n6.92\n16.15\n38.60\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n0\n1\n7.62\n10.98\n0.00\n1.87\n3.24\n8.82\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n0\n1\n111.13\n86.38\n0.00\n35.35\n101.35\n173.10\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n0\n1\n23.38\n23.12\n0.00\n10.31\n16.92\n28.20\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n0\n1\n0.04\n0.13\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n0\n1\n185.49\n34.66\n112.25\n162.07\n177.80\n212.10\n287.75\n▂▇▅▃▁\n\n\nsoil_depth_pelletier\n0\n1\n9.45\n15.25\n0.27\n1.00\n1.20\n6.99\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1\n1.28\n0.27\n0.40\n1.08\n1.42\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.59\n▁▇▂▁▁\n\n\nsoil_conductivity\n0\n1\n1.67\n1.39\n0.45\n0.89\n1.34\n1.89\n10.91\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1\n0.52\n0.15\n0.09\n0.41\n0.53\n0.64\n0.88\n▁▃▆▇▁\n\n\nsand_frac\n0\n1\n35.69\n15.47\n8.18\n24.62\n34.60\n43.77\n91.16\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1\n33.30\n12.82\n4.13\n23.53\n33.65\n42.76\n67.77\n▂▇▇▅▁\n\n\nclay_frac\n0\n1\n20.35\n9.81\n2.08\n13.82\n18.88\n26.58\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1\n0.02\n0.15\n0.00\n0.00\n0.00\n0.00\n1.71\n▇▁▁▁▁\n\n\norganic_frac\n0\n1\n0.45\n2.97\n0.00\n0.00\n0.00\n0.00\n39.37\n▇▁▁▁▁\n\n\nother_frac\n0\n1\n10.88\n16.94\n0.00\n0.00\n2.25\n15.48\n89.87\n▇▂▁▁▁\n\n\ngauge_lat\n0\n1\n39.55\n5.21\n27.05\n36.23\n39.35\n43.96\n48.66\n▂▃▇▅▆\n\n\ngauge_lon\n0\n1\n-98.08\n16.26\n-124.39\n-112.03\n-97.04\n-83.40\n-67.94\n▇▆▇▇▅\n\n\nelev_mean\n0\n1\n842.40\n824.66\n21.75\n278.50\n492.06\n1055.30\n3457.46\n▇▂▁▁▁\n\n\nslope_mean\n0\n1\n50.12\n48.29\n0.82\n7.97\n36.17\n77.62\n255.69\n▇▃▂▁▁\n\n\narea_gages2\n0\n1\n854.89\n1829.26\n4.03\n151.12\n383.82\n855.14\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1\n870.19\n1837.52\n4.10\n164.23\n397.25\n861.21\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1\n0.61\n0.38\n0.00\n0.20\n0.75\n0.97\n1.00\n▅▁▂▂▇\n\n\nlai_max\n0\n1\n3.00\n1.52\n0.37\n1.63\n2.75\n4.60\n5.58\n▅▇▃▅▇\n\n\nlai_diff\n0\n1\n2.27\n1.32\n0.15\n1.07\n2.06\n3.49\n4.82\n▇▇▆▃▆\n\n\ngvf_max\n0\n1\n0.70\n0.17\n0.18\n0.58\n0.75\n0.86\n0.92\n▁▂▃▃▇\n\n\ngvf_diff\n0\n1\n0.31\n0.15\n0.03\n0.18\n0.29\n0.45\n0.65\n▅▇▅▇▂\n\n\ndom_land_cover_frac\n0\n1\n0.80\n0.19\n0.31\n0.64\n0.84\n0.99\n1.00\n▁▂▃▃▇\n\n\nroot_depth_50\n0\n1\n0.18\n0.03\n0.12\n0.16\n0.18\n0.19\n0.25\n▃▅▇▂▂\n\n\nroot_depth_99\n0\n1\n1.81\n0.30\n1.50\n1.51\n1.79\n2.00\n3.10\n▇▃▂▁▁\n\n\n\n\nvisdat::vis_miss(camels_clean)\n\n\n\n\n\n\n\n\nyay, the data is now squeaky clean!\n\nhead(camels_clean)\n\n# A tibble: 6 × 58\n  gauge_id p_mean pet_mean p_seasonality frac_snow aridity high_prec_freq\n  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1 01022500   3.61     2.12        -0.115     0.245   0.587           20.6\n2 01031500   3.52     2.07         0.104     0.292   0.588           18.9\n3 01047000   3.32     2.09         0.148     0.280   0.629           20.1\n4 01052500   3.73     2.10         0.152     0.353   0.562           13.5\n5 01054200   4.07     2.13         0.105     0.300   0.523           17.5\n6 01055000   3.49     2.09         0.167     0.306   0.599           19.2\n# ℹ 51 more variables: high_prec_dur &lt;dbl&gt;, high_prec_timing &lt;chr&gt;,\n#   low_prec_freq &lt;dbl&gt;, low_prec_dur &lt;dbl&gt;, low_prec_timing &lt;chr&gt;,\n#   geol_1st_class &lt;chr&gt;, glim_1st_class_frac &lt;dbl&gt;, geol_2nd_class &lt;chr&gt;,\n#   glim_2nd_class_frac &lt;dbl&gt;, carbonate_rocks_frac &lt;dbl&gt;, geol_porostiy &lt;dbl&gt;,\n#   geol_permeability &lt;dbl&gt;, q_mean &lt;dbl&gt;, runoff_ratio &lt;dbl&gt;, slope_fdc &lt;dbl&gt;,\n#   baseflow_index &lt;dbl&gt;, stream_elas &lt;dbl&gt;, q5 &lt;dbl&gt;, q95 &lt;dbl&gt;,\n#   high_q_freq &lt;dbl&gt;, high_q_dur &lt;dbl&gt;, low_q_freq &lt;dbl&gt;, low_q_dur &lt;dbl&gt;, …\n\n\n\n#Be sure to set a seed to ensure the random process is reproducable.\nset.seed(305)\n\n#Use the initial_split() function from the rsample package to split the data. Use 80% of the data for training and 20% for testing.\ndata_split &lt;- initial_split(camels_clean, prop = 0.8)\n\n#Use the training() and testing() functions from the rsample package to extract the training and testing data.frames.\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n\nglimpse(train_data)\n\nRows: 405\nColumns: 58\n$ gauge_id             &lt;chr&gt; \"06353000\", \"02450250\", \"05393500\", \"08082700\", \"…\n$ p_mean               &lt;dbl&gt; 1.135228, 4.409546, 2.388423, 1.827210, 1.332784,…\n$ pet_mean             &lt;dbl&gt; 3.043100, 3.063883, 2.086423, 3.651841, 3.644738,…\n$ p_seasonality        &lt;dbl&gt; 0.77953446, -0.17475364, 0.58381925, 0.37758094, …\n$ frac_snow            &lt;dbl&gt; 0.155792226, 0.014286411, 0.196169866, 0.01280213…\n$ aridity              &lt;dbl&gt; 2.6806068, 0.6948296, 0.8735569, 1.9985882, 2.734…\n$ high_prec_freq       &lt;dbl&gt; 23.45, 21.10, 22.30, 26.45, 24.55, 25.85, 24.30, …\n$ high_prec_dur        &lt;dbl&gt; 1.443077, 1.241176, 1.238889, 1.410667, 1.825279,…\n$ high_prec_timing     &lt;chr&gt; \"jja\", \"djf\", \"jja\", \"jja\", \"djf\", \"son\", \"djf\", …\n$ low_prec_freq        &lt;dbl&gt; 300.55, 256.30, 253.65, 313.00, 317.10, 297.50, 2…\n$ low_prec_dur         &lt;dbl&gt; 8.079301, 4.808630, 4.010277, 10.178862, 15.17224…\n$ low_prec_timing      &lt;chr&gt; \"djf\", \"son\", \"djf\", \"djf\", \"jja\", \"mam\", \"son\", …\n$ geol_1st_class       &lt;chr&gt; \"Unconsolidated sediments\", \"Siliciclastic sedime…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.9467475, 0.8498731, 0.9999600, 0.6826415, 0.852…\n$ geol_2nd_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Carbonate sed…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.052826180, 0.150117891, 0.000040049, 0.31735847…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.0000000000, 0.1501178911, 0.0000000000, 0.00000…\n$ geol_porostiy        &lt;dbl&gt; 0.2167, 0.1705, 0.0100, 0.2080, 0.1853, 0.2165, 0…\n$ geol_permeability    &lt;dbl&gt; -13.1298, -14.6896, -14.1000, -14.2036, -14.7006,…\n$ q_mean               &lt;dbl&gt; 0.04258435, 1.75247305, 0.98549680, 0.07074501, 0…\n$ runoff_ratio         &lt;dbl&gt; 0.03751172, 0.39742714, 0.41261401, 0.03871750, 0…\n$ slope_fdc            &lt;dbl&gt; 0.5000052, 1.5627891, 1.0920664, 0.0000000, 0.375…\n$ baseflow_index       &lt;dbl&gt; 0.26867566, 0.37347985, 0.40953870, 0.04222529, 0…\n$ stream_elas          &lt;dbl&gt; 2.7207228, 1.4043343, 1.7390199, 3.2818758, 2.527…\n$ q5                   &lt;dbl&gt; 0.0000000000, 0.0401026057, 0.0821296454, 0.00000…\n$ q95                  &lt;dbl&gt; 0.12863884, 6.65914321, 4.11536115, 0.08402237, 0…\n$ high_q_freq          &lt;dbl&gt; 53.15, 26.95, 26.15, 102.75, 48.55, 40.35, 40.15,…\n$ high_q_dur           &lt;dbl&gt; 13.805195, 2.495370, 4.547826, 14.075342, 10.7888…\n$ low_q_freq           &lt;dbl&gt; 249.30, 157.75, 98.75, 326.80, 254.45, 245.10, 18…\n$ low_q_dur            &lt;dbl&gt; 47.942308, 20.095541, 16.056911, 45.706294, 57.17…\n$ zero_q_freq          &lt;dbl&gt; 0.119917864, 0.000000000, 0.000000000, 0.71868583…\n$ hfd_mean             &lt;dbl&gt; 201.15, 143.75, 203.80, 241.50, 137.25, 205.55, 1…\n$ soil_depth_pelletier &lt;dbl&gt; 1.8246803, 1.0393258, 36.1295337, 4.8914141, 2.29…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.4766569, 0.6025877, 1.5000000, 1.5000000, 0.894…\n$ soil_porosity        &lt;dbl&gt; 0.4466958, 0.4377834, 0.4228323, 0.4504625, 0.452…\n$ soil_conductivity    &lt;dbl&gt; 1.0020886, 1.8349041, 2.5500942, 0.6191830, 0.808…\n$ max_water_content    &lt;dbl&gt; 0.3793515, 0.1613655, 0.6084069, 0.7036729, 0.339…\n$ sand_frac            &lt;dbl&gt; 19.47690, 29.22051, 55.23465, 25.28627, 22.72104,…\n$ silt_frac            &lt;dbl&gt; 22.35083, 28.51203, 34.56309, 24.83140, 33.26105,…\n$ clay_frac            &lt;dbl&gt; 15.808922, 9.194150, 10.093263, 50.353912, 31.572…\n$ water_frac           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ organic_frac         &lt;dbl&gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,…\n$ other_frac           &lt;dbl&gt; 42.348131, 33.210477, 0.000000, 0.000000, 15.4132…\n$ gauge_lat            &lt;dbl&gt; 46.09167, 34.28538, 45.44913, 33.32927, 36.26802,…\n$ gauge_lon            &lt;dbl&gt; -101.33374, -87.39891, -89.97931, -99.46508, -121…\n$ elev_mean            &lt;dbl&gt; 767.71, 254.92, 487.07, 453.85, 535.53, 88.45, 95…\n$ slope_mean           &lt;dbl&gt; 9.54494, 12.27321, 7.18561, 4.04181, 56.57053, 5.…\n$ area_gages2          &lt;dbl&gt; 4526.51, 231.83, 220.44, 276.04, 608.23, 925.44, …\n$ area_geospa_fabric   &lt;dbl&gt; 4605.23, 242.69, 225.96, 276.84, 648.41, 925.53, …\n$ frac_forest          &lt;dbl&gt; 0.0001, 0.8413, 1.0000, 0.0000, 0.1373, 0.1949, 0…\n$ lai_max              &lt;dbl&gt; 1.2441058, 5.1548848, 4.7766187, 0.8390016, 1.240…\n$ lai_diff             &lt;dbl&gt; 1.0571745, 4.4509656, 4.2894818, 0.4799003, 0.841…\n$ gvf_max              &lt;dbl&gt; 0.4916691, 0.8849997, 0.8835059, 0.3803392, 0.479…\n$ gvf_diff             &lt;dbl&gt; 0.3504405, 0.4502583, 0.5458937, 0.1217240, 0.219…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.6898734, 0.7550442, 0.7769249, 0.9878854, 0.493…\n$ dom_land_cover       &lt;chr&gt; \"    Grasslands\", \"    Deciduous Broadleaf Forest…\n$ root_depth_50        &lt;dbl&gt; 0.1386076, 0.2041875, 0.1877692, 0.1207269, 0.152…\n$ root_depth_99        &lt;dbl&gt; 1.500000, 2.091427, 1.888462, 1.500000, 2.088358,…\n\n\n\n#Use the recipe() function from the recipes package to create a recipe object. The formula should be based on how you want to predict q_mean and the data should be the training data. Remember to apply any data transformations you deem needed, and be sure not to transform the outcome variable directly in the recipe.\ncamels_recipe &lt;- recipe(q_mean ~ ., data = train_data) %&gt;%\n#You should not use gauge_lat and gauge_lon in the recipe as predictors. You can use the step_rm() function to remove them from the recipe while ensureing they persist in any data passed throuhg fit_*\n\n  step_rm(gauge_lat, gauge_lon)\n\n\n#1.\n#  resample the data using the vfold_cv() function to generate 10 k-fold samples for cross-validation\"\n\nset.seed(305)\nfolds &lt;- vfold_cv(train_data, v = 10)\n\n\n#2\n# Define 3 models that you feel have the best chance of performing well on the data. \n\n# Random forest\nrf_model &lt;- rand_forest(mtry = 5, min_n = 10, trees = 500) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n# Decision tree\ndt_model &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\n# boosted trees\nbt_model &lt;- bag_tree(min_n = tune()) %&gt;%\n  set_engine(\"rpart\", times = 25) %&gt;%\n  set_mode(\"regression\")\n\n\n# Create the workflow set for regression\n\n\nwf_rf &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_recipe(camels_recipe)\n\nwf_dt &lt;- workflow() %&gt;%\n  add_model(dt_model) %&gt;%\n  add_recipe(camels_recipe)\n\nwf_bt &lt;- workflow() %&gt;%\n  add_model(bt_model) %&gt;%\n  add_recipe(camels_recipe)\n\nmodel_set &lt;- workflow_set(\n  preproc = list(\n    rf_recipe = camels_recipe,\n    dt_recipe = camels_recipe,\n    bt_recipe = camels_recipe\n  ),\n  models = list(\n    rf = rf_model,\n    dt = dt_model,\n    bt = bt_model\n  )\n)\n\n\n# Tune across the set\nset.seed(305)\nmodel_results &lt;- workflow_map(\n  model_set,\n  resamples = folds,\n  metrics = metric_set(rmse, rsq),\n  grid = 20,         # 20 random grid points\n  verbose = TRUE,\n  seed = 305,\n  fn = \"tune_grid\"\n)\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 1 of 3 resampling: rf_recipe_rf\n\n\nWarning: package 'ranger' was built under R version 4.4.3\n\n\n✔ 1 of 3 resampling: rf_recipe_rf (2.1s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 2 of 3 resampling: rf_recipe_dt\n\n\n✔ 2 of 3 resampling: rf_recipe_dt (690ms)\n\n\ni 3 of 3 tuning:     rf_recipe_bt\n\n\n✔ 3 of 3 tuning:     rf_recipe_bt (2m 12.4s)\n\n\n\n#, use autoplot to visualize the results of the workflow set.\n\nautoplot(model_results)\n\n\n\n\n\n\n\n\nBased on the visualized metrics, the bagged trees (bag_tree) model clearly demonstrates the best performance overall. In the RMSE panel on the left, its top-performing trials achieve values just under 0.3, outperforming the random forest (around 0.35) and the decision tree (which is slightly above 0.5). Similarly, in the R² panel on the right, the best bagged tree models reach values close to 0.975, slightly higher than the random forest (around 0.96) and significantly better than the decision tree (below 0.93). While the red points for bagged trees represent multiple tuned versions, it’s evident that the top-performing configurations outperform the other models in both accuracy and explanatory power.\nI chose bagged trees because it achieved the best results.\n\n#Define a tunable model\ntunable_model&lt;- bag_tree(\n  #specify at least 2 hyperparameters to tune using    the tune() function\n  min_n = tune(),         \n  cost_complexity = tune()\n) %&gt;%\n  set_engine(\"rpart\", times = 25) %&gt;%\n  set_mode(\"regression\")\n\n\n#Create a workflow object using the workflow() that adds your recipe and tunable model.\nwf_bag_tune &lt;- workflow() %&gt;%\n  add_model(tunable_model) %&gt;%\n  add_recipe(camels_recipe)\n\n\n#Use the extract_parameter_set_dials(YOUR MODEL WORKFLOW) and save it to an object named dials\ndials &lt;- extract_parameter_set_dials(wf_bag_tune)\n\n# Check the dials$object slot to see the tunable parameters and their ranges.\ndials$object\n\n[[1]]\n\n\nCost-Complexity Parameter (quantitative)\n\n\nTransformer: log-10 [1e-100, Inf]\n\n\nRange (transformed scale): [-10, -1]\n\n\n\n[[2]]\n\n\nMinimal Node Size (quantitative)\n\n\nRange: [2, 40]\n\n\nBetween 10^-10 and 10^-1 trees were trimed and between 2-40 observations are required for a split. This is good.\n\n#Create a SFD Grid Object with 25 predefined combinations.\nSFD_grid &lt;- grid_space_filling(\n  dials,\n  size = 25\n)\n\n\nmodel_params &lt;- tune_grid(\n  wf_bag_tune,          \n  resamples = folds,    \n  grid = SFD_grid,      \n  metrics = metric_set(rmse, rsq, mae),   \n  control = control_grid(save_pred = TRUE) \n)\n\n\nautoplot(model_params)\n\n\n\n\n\n\n\n\nbest-performing bagged tree models appear in the region with low cost-complexity values (between -10 and -7.5 on the log scale) and smaller minimal node sizes (between 5 and 15). These configurations have the best RMSE and MAE,showing that these models explain nearly all of the variance in the target variable.\n\n#Use the collect_metrics() function to check the skill of the tuned model. Describe what you see, remember dplyr functions like arrange, slice_*, and filter will work on this tibble.\ncollect_metrics(model_params)\n\n# A tibble: 75 × 8\n   cost_complexity min_n .metric .estimator  mean     n std_err .config         \n             &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1        1   e-10    24 mae     standard   0.185    10 0.0120  Preprocessor1_M…\n 2        1   e-10    24 rmse    standard   0.331    10 0.0325  Preprocessor1_M…\n 3        1   e-10    24 rsq     standard   0.962    10 0.00472 Preprocessor1_M…\n 4        2.37e-10    13 mae     standard   0.171    10 0.0124  Preprocessor1_M…\n 5        2.37e-10    13 rmse    standard   0.295    10 0.0283  Preprocessor1_M…\n 6        2.37e-10    13 rsq     standard   0.968    10 0.00474 Preprocessor1_M…\n 7        5.62e-10    33 mae     standard   0.195    10 0.0142  Preprocessor1_M…\n 8        5.62e-10    33 rmse    standard   0.363    10 0.0375  Preprocessor1_M…\n 9        5.62e-10    33 rsq     standard   0.958    10 0.00507 Preprocessor1_M…\n10        1.33e- 9     5 mae     standard   0.168    10 0.0103  Preprocessor1_M…\n# ℹ 65 more rows\n\n#Use the show_best() function to show the best performing model based on Mean Absolute Error.\nshow_best(model_params, metric = \"mae\")\n\n# A tibble: 5 × 8\n  cost_complexity min_n .metric .estimator  mean     n std_err .config          \n            &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1   0.0000000422     11 mae     standard   0.167    10  0.0113 Preprocessor1_Mo…\n2   0.0001            2 mae     standard   0.167    10  0.0107 Preprocessor1_Mo…\n3   0.00000000133     5 mae     standard   0.168    10  0.0103 Preprocessor1_Mo…\n4   0.00000000316    19 mae     standard   0.169    10  0.0146 Preprocessor1_Mo…\n5   0.00000750       16 mae     standard   0.171    10  0.0107 Preprocessor1_Mo…\n\n#Please interpret the results of the first row of show_best(). What do you see? What hyperparameter set is best for this model, based on MAE?\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n\n#Use the select_best() function to save the best performing hyperparameter set to an object called hp_best.\ncollect_metrics(model_params)\n\n# A tibble: 75 × 8\n   cost_complexity min_n .metric .estimator  mean     n std_err .config         \n             &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1        1   e-10    24 mae     standard   0.185    10 0.0120  Preprocessor1_M…\n 2        1   e-10    24 rmse    standard   0.331    10 0.0325  Preprocessor1_M…\n 3        1   e-10    24 rsq     standard   0.962    10 0.00472 Preprocessor1_M…\n 4        2.37e-10    13 mae     standard   0.171    10 0.0124  Preprocessor1_M…\n 5        2.37e-10    13 rmse    standard   0.295    10 0.0283  Preprocessor1_M…\n 6        2.37e-10    13 rsq     standard   0.968    10 0.00474 Preprocessor1_M…\n 7        5.62e-10    33 mae     standard   0.195    10 0.0142  Preprocessor1_M…\n 8        5.62e-10    33 rmse    standard   0.363    10 0.0375  Preprocessor1_M…\n 9        5.62e-10    33 rsq     standard   0.958    10 0.00507 Preprocessor1_M…\n10        1.33e- 9     5 mae     standard   0.168    10 0.0103  Preprocessor1_M…\n# ℹ 65 more rows\n\nshow_best(model_params, metric = \"mae\")\n\n# A tibble: 5 × 8\n  cost_complexity min_n .metric .estimator  mean     n std_err .config          \n            &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1   0.0000000422     11 mae     standard   0.167    10  0.0113 Preprocessor1_Mo…\n2   0.0001            2 mae     standard   0.167    10  0.0107 Preprocessor1_Mo…\n3   0.00000000133     5 mae     standard   0.168    10  0.0103 Preprocessor1_Mo…\n4   0.00000000316    19 mae     standard   0.169    10  0.0146 Preprocessor1_Mo…\n5   0.00000750       16 mae     standard   0.171    10  0.0107 Preprocessor1_Mo…\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n\nThe best-performing model based on Mean Absolute Error (MAE) was achieved with a cost_complexity value of 3.16 × 10⁻⁹ and a minimum node size (min_n) of 19.\n\n# FRun finalize_workflow() based on your workflow and best hyperparmater set to create a final workflow object.\nfinal_wf &lt;- finalize_workflow(\n  wf_bag_tune,  \n  hp_best     \n)\n\n\n#Use last_fit() to fit the finalized workflow the original split object (output of initial_split()). This will fit the model to the training data and validate it on the testing data.\nfinal_fit &lt;- last_fit(\n  final_wf,      \n  split = data_split)\n  \n#Use the collect_metrics() function to check the performance of the final model on the test data. This will return a tibble with the metrics for the final model.\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.385 Preprocessor1_Model1\n2 rsq     standard       0.939 Preprocessor1_Model1\n\n\nThe RMSE is approximately 0.359 the model’s predictions deviate from the true values by about 0.36 units of q_mean. Meanwhile, the R² value is around 0.947, meaning that the model explains about 94.7% of the variance in the test. This is very good.\n\n# Use the collect_predictions() function to check the predictions of the final model on the test data. This will return a tibble with the predictions for the final model.\npredictions &lt;- collect_predictions(final_fit)\n\n\n# Use the output of this to create a scatter plot of the predicted values vs the actual values. Use the ggplot2 package to create the plot. This plot should include (1) geom_smooth(method = “lm”) to add the linear fit of predictions and truth (2) geom_abline() to add a 1:1 line (3) nice colors via scale_color_* and (4) accurate labels.\nlibrary(ggplot2)\n\nggplot(predictions, aes(x = .pred, y = q_mean)) +\n  geom_point(aes(color = .pred), alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"skyblue\", linetype = \"dashed\") +\n  geom_abline(slope = 1, intercept = 0, color = \"deeppink\", linetype = \"solid\") +\n   scale_color_gradient(\n    low = \"lightblue\",  \n    high = \"pink\")+\n  labs(\n    title = \"Predicted vs Actual Values for Final Bagged Tree Model\",\n    x = \"Predicted q_mean\",\n    y = \"Actual q_mean\",\n    color = \"Predicted Value\"\n  ) +\n  theme_dark()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n#This full fit can be passed to the augment() function to make predictions on the full, cleaned data. This will return a tibble with the predictions for the full data\nfinal_model_full &lt;- fit(\n  final_wf,\n  data = camels_clean\n)\n\n#Use the mutate() function to calculate the residuals of the predictions. The residuals are the difference between the predicted values and the actual values squared.\n# Fit the final model to the full cleaned data\nfinal_model_full &lt;- fit(\n  final_wf,\n  data = camels_clean\n)\n\n# Get predictions and retain true values\nfull_predictions &lt;- augment(\n  final_model_full,\n  new_data = camels_clean\n)\n\n# Compute residuals\nfull_predictions &lt;- full_predictions %&gt;%\n  mutate(residual = (.pred - q_mean)^2)\n\n\n#Use ggplot2 to create a map of the predictions\n\nmap_pred &lt;- ggplot(full_predictions, aes(x = gauge_lon, y = gauge_lat)) +\n  geom_point(aes(color = .pred), size = 2) +\n  scale_color_gradient(low = \"lightblue\", high = \"darkred\") +\n  coord_fixed(1.3) +\n  labs(\n    title = \"Predicted q_mean Across CONUS\",\n    color = \"Predicted q_mean\"\n  ) +\n  theme_minimal()\n#Use ggplot2 to create a map of the residuals.\nmap_resid &lt;- ggplot(full_predictions, aes(x = gauge_lon, y = gauge_lat)) +\n  geom_point(aes(color = residual), size = 2) +\n  scale_color_gradient(low = \"lightpink\", high = \"navy\") +\n  coord_fixed(1.3) +\n  labs(\n    title = \"Residuals Across CONUS\",\n    color = \"Squared Error\"\n  ) +\n  theme_minimal()\n#Use patchwork to combine the two maps into one figure.\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\nmap_pred + map_resid"
  }
]