[
  {
    "objectID": "Lab 6.html",
    "href": "Lab 6.html",
    "title": "Lab6",
    "section": "",
    "text": "#Question 1: Download data ##Set Up\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nWarning: package 'yardstick' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf', \n              mode = 'wb')\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\n# merge the data into a single data frame\ncamels &lt;- reduce(camels, power_full_join, by = 'gauge_id')\n\n\nlist.files(\"data\")\n\n[1] \"camels_attributes_v2.0.pdf\" \"camels_clim.txt\"           \n[3] \"camels_geol.txt\"            \"camels_hydro.txt\"          \n[5] \"camels_soil.txt\"            \"camels_topo.txt\"           \n[7] \"camels_vege.txt\"           \n\n\n###yay it worked! all 6 data files + the PDF are here\n#Question 2:Make 2 maps\n\np1 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\np1\n\n\n\n\n\n\n\n\n###It looks just like the map in the assignment!\n\n# First map: Color points by aridity\np2 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"pink\", high = \"navy\") +\n  labs(color = \"Aridity Index\") +\n  ggthemes::theme_map()\n\n# Second map: Color points by p_mean (mean annual precipitation)\np3 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"lightblue\", high =\"purple\") +\n  labs (color = \"p_mean (mm)\") +\n  ggthemes::theme_map()\n\n\np2 + p3\n\n\n\n\n\n\n\n\n##Model Preparation\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n###Looks like the expected results from the assignments.\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###The relationship is seen here just like in the assignment. There is a relationship between rainfall but it is not linear.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###Log scaled made a more leniar relationship, but clustared and unevenly distributed.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###Definitely more evenly spread after being skewed, just like expected.\n##Model building ###set seed for reproducabilty:\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n###use 80% of the data for training and 20% for testing with no stratification.\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n### 10-fold cross validation dataset to help us evaluate multi-model setups.\n\n###Separately we have used the recipe function to define a series of data preprocessing steps:\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n###fitting a linear model to the data.\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n###Just as expected\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n###Just as expected again\n##A Model’s ability to predict new data (incorrect tests) ###Using the wrong verion augment function to add predicted values to the test data won’t work because the recipe has not been applied\n\nnrow(camels_test)\n\n[1] 135\n\nnrow(camels_train)\n\n[1] 536\n\n\n###look how many more test vs train\n\n#broom::augment(lm_base, data = camels_test)\n\n###As expected, there is an erro, I will put it behind a “#” so that I can render this assignment.\n###Using the predict function to directly test the data without recipe onject we also see issues\n\ncamels_test$p2 = predict(lm_base, newdata = camels_test)\n\n## Scales way off!\nggplot(camels_test, aes(x = p2, y = logQmean)) + \n  geom_point() + \n  # Linear fit line, no error bands\n  geom_smooth(method = \"lm\", se = FALSE, size =1) +\n  # 1:1 line\n  geom_abline(color = \"red\", size = 1) + \n  labs(title = \"Linear Model Using `predict()`\",\n       x = \"Predicted Log Mean Flow\",\n       y = \"Observed Log Mean Flow\") + \n  theme_linedraw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###As expected.\n##Correct version ###prep -&gt; bake -&gt; predict; using the prep and bake functions with the recipe object to make a prediction.\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n##Evaluating the model ###calculates common regression metrics such as RMSE, R-squared, and MAE between the observed and predicted values with metrics.\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n###model of the observed vs predicted\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"pink\", mid = \"purple\", high = \"navy\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n###This really was a lot of work for one single fragile graph that can’t test other algarithsms.\n##Better approach: Workflow\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n###Now workflow is embedded in the model!\n##Make Predictions:\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  62\n\n\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n###Boom! an easy to make and more adaptble graph from my data!\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  61\n\n\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.596\n2 rsq     standard       0.733\n3 mae     standard       0.370\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nHere is the proof, using the framwork for a completely new model!\n##Workflowset approach\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.565  0.0243    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0255    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\n#Question 3: Build a xgboost and neural network model using boost_tree\n\nxgb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n##build a neural network model using the nnet engine from the baguette package using the bag_mlp function\n\nnnet_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n##Add this to the above workflow\n\nwf_results &lt;- workflow_set(\n  preproc = list(camels_recipe = rec),\n  models = list(\n    lm_model = lm_model,\n    rf_model = rf_model,\n    xgb_model = xgb_model,\n    nnet_model = nnet_model\n  )\n)\n\n# Now apply fit_resamples and ASSIGN it\nwf_results &lt;- workflow_map(wf_results, \"fit_resamples\", resamples = camels_cv)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n##Evaluate the model and compare it to the linear and random forest models\n\ncollect_metrics(wf_results)\n\n# A tibble: 8 × 9\n  wflow_id          .config preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 camels_recipe_lm… Prepro… recipe  line… rmse    standard   0.569    10  0.0260\n2 camels_recipe_lm… Prepro… recipe  line… rsq     standard   0.770    10  0.0223\n3 camels_recipe_rf… Prepro… recipe  rand… rmse    standard   0.565    10  0.0253\n4 camels_recipe_rf… Prepro… recipe  rand… rsq     standard   0.770    10  0.0264\n5 camels_recipe_xg… Prepro… recipe  boos… rmse    standard   0.600    10  0.0289\n6 camels_recipe_xg… Prepro… recipe  boos… rsq     standard   0.745    10  0.0268\n7 camels_recipe_nn… Prepro… recipe  bag_… rmse    standard   0.547    10  0.0309\n8 camels_recipe_nn… Prepro… recipe  bag_… rsq     standard   0.787    10  0.0267\n\n\n###Now I have the model results, I can run them through an evaluation to see what I will move forward with:\n\nrank_results(wf_results, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 camels_recipe_nn… Prepro… rmse    0.547  0.0309    10 recipe       bag_…     1\n2 camels_recipe_nn… Prepro… rsq     0.787  0.0267    10 recipe       bag_…     1\n3 camels_recipe_rf… Prepro… rmse    0.565  0.0253    10 recipe       rand…     2\n4 camels_recipe_rf… Prepro… rsq     0.770  0.0264    10 recipe       rand…     2\n5 camels_recipe_lm… Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 camels_recipe_lm… Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 camels_recipe_xg… Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 camels_recipe_xg… Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\n\n##Which of the 4 models would you move forward with? ###I will move forward with the neural network model as it performed best in cross-validation.\n#Question 4a: Data Prep / Data Splitting ###Borrowing from the workflow presented above, build your own complete ML pipeline to predict mean streamflow using the CAMELS dataset. A successful model will have a R-squared value &gt; 0.9.\n\nset.seed(13)\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n##Data Spliting ###Set a seed for reproducible Create an initial split with 75% used for training and 25% for testing Extract your training and testing sets Build a 10-fold CV dataset as well\n\ncamels_split_4 &lt;- initial_split(camels, prop = 0.75)\ncamels_train_4 &lt;- training(camels_split_4)\ncamels_test_4  &lt;- testing(camels_split_4)\n\ncamels_cv_4 &lt;- vfold_cv(camels_train_4, v = 10)\n\n#Question 4b: Recipe ###Define a formula you want to use to predict logQmean\n##Describe in words why you are choosing the formula you are. Consult the downloaded PDF.\n######I chose aridity and p_mean as predictors based the CAMELS showing that precipitation and aridity directly influence streamflow.\n##for the data to help you make this decision. Build a recipe that you feel handles the predictors chosen well\n\nrec_4 &lt;- recipe(logQmean ~ aridity + p_mean, data = camels_train_4) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n#Question 4c: Define 3 models ###Define a random forest model using the rand_forest function Set the engine to ranger and the mode to regression Define two other models of your choice\n\nlm_model_4 &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\nrf_model_4 &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n###I need a thrid model… ###I will move forward with recipe_nnet_model was the most accurate in Q3, so I will use it here.\n\nnnet_model_4 &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n#Question 4d: workflow set ###Create a workflow object Add the recipe Add the model(s) Fit the model to the resamples\n\nwf_set_4 &lt;- workflow_set(\n  preproc = list(camels_recipe_4 = rec_4),  # named recipe\n  models = list(\n    lm_model   = lm_model_4,\n    rf_model   = rf_model_4,\n    nnet_model = nnet_model_4\n  )\n) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv_4)\n\n#Question 4e: Evaluation ###Use autoplot and rank_results to compare the models. Describe what model you think is best and why!\n\nrank_results(wf_set_4, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 camels_recipe_4_… Prepro… rmse    0.543  0.0304    10 recipe       bag_…     1\n2 camels_recipe_4_… Prepro… rsq     0.783  0.0252    10 recipe       bag_…     1\n3 camels_recipe_4_… Prepro… rmse    0.551  0.0323    10 recipe       rand…     2\n4 camels_recipe_4_… Prepro… rsq     0.772  0.0280    10 recipe       rand…     2\n5 camels_recipe_4_… Prepro… rmse    0.570  0.0339    10 recipe       line…     3\n6 camels_recipe_4_… Prepro… rsq     0.762  0.0253    10 recipe       line…     3\n\n\n\nautoplot(wf_set_4)\n\n\n\n\n\n\n\n\n\n\nAs we can see, camels_recipe_4_nnet_model is the best model again in terms of rsq.\n#Question 4f: Extract and Evaluate ###Build a workflow (not workflow set) with your favorite model, recipe, and training data Use fit to fit all training data to the model Use augment to make predictions on the test data Create a plot of the observed vs predicted values with clear title, axis labels, and a compelling color scale Describe what you think of the results!\n\nfinal_nnet_wf_4 &lt;- extract_workflow(wf_set_4, \"camels_recipe_4_nnet_model\")\nfinal_nnet_wf_4 &lt;- final_nnet_wf_4 %&gt;%\n  fit(data = camels_train_4)\n\nnnet_test_preds_4 &lt;- augment(final_nnet_wf_4, new_data = camels_test_4)\n\n# View metrics\nmetrics(nnet_test_preds_4, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.616\n2 rsq     standard       0.743\n3 mae     standard       0.360\n\n\n\nggplot(nnet_test_preds_4, aes(x = logQmean, y = .pred, color = aridity)) +\n  geom_point(alpha = 0.7) +\n  geom_abline(linetype = \"dashed\", color = \"red\") +\n  scale_color_viridis_c(trans = \"log\") +\n  theme_linedraw() +\n  labs(\n    title = \"Neural Net: Observed vs Predicted Log Mean Flow\",\n    x = \"Observed Log Mean Flow\",\n    y = \"Predicted Log Mean Flow\",\n    color = \"Aridity (log scale)\"\n  )\n\n\n\n\n\n\n\n\n###I think the results are slighlty scattered but far ess and more evenly spaced than the other prediction models graphed."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lab6",
    "section": "",
    "text": "#Question 1: Download data ##Set Up\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nWarning: package 'yardstick' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf', \n              mode = 'wb')\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\n# merge the data into a single data frame\ncamels &lt;- reduce(camels, power_full_join, by = 'gauge_id')\n\n\nlist.files(\"data\")\n\n[1] \"camels_attributes_v2.0.pdf\" \"camels_clim.txt\"           \n[3] \"camels_geol.txt\"            \"camels_hydro.txt\"          \n[5] \"camels_soil.txt\"            \"camels_topo.txt\"           \n[7] \"camels_vege.txt\"           \n\n\n###yay it worked! all 6 data files + the PDF are here\n#Question 2:Make 2 maps\n\np1 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\np1\n\n\n\n\n\n\n\n\n###It looks just like the map in the assignment!\n\n# First map: Color points by aridity\np2 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"pink\", high = \"navy\") +\n  labs(color = \"Aridity Index\") +\n  ggthemes::theme_map()\n\n# Second map: Color points by p_mean (mean annual precipitation)\np3 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"lightblue\", high =\"purple\") +\n  labs (color = \"p_mean (mm)\") +\n  ggthemes::theme_map()\n\n\np2 + p3\n\n\n\n\n\n\n\n\n##Model Preparation\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n###Looks like the expected results from the assignments.\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###The relationship is seen here just like in the assignment. There is a relationship between rainfall but it is not linear.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###Log scaled made a more leniar relationship, but clustared and unevenly distributed.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###Definitely more evenly spread after being skewed, just like expected.\n##Model building ###set seed for reproducabilty:\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n###use 80% of the data for training and 20% for testing with no stratification.\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n### 10-fold cross validation dataset to help us evaluate multi-model setups.\n\n###Separately we have used the recipe function to define a series of data preprocessing steps:\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n###fitting a linear model to the data.\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n###Just as expected\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n###Just as expected again\n##A Model’s ability to predict new data (incorrect tests) ###Using the wrong verion augment function to add predicted values to the test data won’t work because the recipe has not been applied\n\nnrow(camels_test)\n\n[1] 135\n\nnrow(camels_train)\n\n[1] 536\n\n\n###look how many more test vs train\n\n#broom::augment(lm_base, data = camels_test)\n\n###As expected, there is an erro, I will put it behind a “#” so that I can render this assignment.\n###Using the predict function to directly test the data without recipe onject we also see issues\n\ncamels_test$p2 = predict(lm_base, newdata = camels_test)\n\n## Scales way off!\nggplot(camels_test, aes(x = p2, y = logQmean)) + \n  geom_point() + \n  # Linear fit line, no error bands\n  geom_smooth(method = \"lm\", se = FALSE, size =1) +\n  # 1:1 line\n  geom_abline(color = \"red\", size = 1) + \n  labs(title = \"Linear Model Using `predict()`\",\n       x = \"Predicted Log Mean Flow\",\n       y = \"Observed Log Mean Flow\") + \n  theme_linedraw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n###As expected.\n##Correct version ###prep -&gt; bake -&gt; predict; using the prep and bake functions with the recipe object to make a prediction.\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n##Evaluating the model ###calculates common regression metrics such as RMSE, R-squared, and MAE between the observed and predicted values with metrics.\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n###model of the observed vs predicted\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"pink\", mid = \"purple\", high = \"navy\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n###This really was a lot of work for one single fragile graph that can’t test other algarithsms.\n##Better approach: Workflow\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n###Now workflow is embedded in the model!\n##Make Predictions:\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  62\n\n\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n###Boom! an easy to make and more adaptble graph from my data!\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  61\n\n\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.596\n2 rsq     standard       0.733\n3 mae     standard       0.370\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nHere is the proof, using the framwork for a completely new model!\n##Workflowset approach\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.565  0.0243    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0255    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\n#Question 3: Build a xgboost and neural network model using boost_tree\n\nxgb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n##build a neural network model using the nnet engine from the baguette package using the bag_mlp function\n\nnnet_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n##Add this to the above workflow\n\nwf_results &lt;- workflow_set(\n  preproc = list(camels_recipe = rec),\n  models = list(\n    lm_model = lm_model,\n    rf_model = rf_model,\n    xgb_model = xgb_model,\n    nnet_model = nnet_model\n  )\n)\n\n# Now apply fit_resamples and ASSIGN it\nwf_results &lt;- workflow_map(wf_results, \"fit_resamples\", resamples = camels_cv)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n##Evaluate the model and compare it to the linear and random forest models\n\ncollect_metrics(wf_results)\n\n# A tibble: 8 × 9\n  wflow_id          .config preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 camels_recipe_lm… Prepro… recipe  line… rmse    standard   0.569    10  0.0260\n2 camels_recipe_lm… Prepro… recipe  line… rsq     standard   0.770    10  0.0223\n3 camels_recipe_rf… Prepro… recipe  rand… rmse    standard   0.565    10  0.0253\n4 camels_recipe_rf… Prepro… recipe  rand… rsq     standard   0.770    10  0.0264\n5 camels_recipe_xg… Prepro… recipe  boos… rmse    standard   0.600    10  0.0289\n6 camels_recipe_xg… Prepro… recipe  boos… rsq     standard   0.745    10  0.0268\n7 camels_recipe_nn… Prepro… recipe  bag_… rmse    standard   0.547    10  0.0309\n8 camels_recipe_nn… Prepro… recipe  bag_… rsq     standard   0.787    10  0.0267\n\n\n###Now I have the model results, I can run them through an evaluation to see what I will move forward with:\n\nrank_results(wf_results, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 camels_recipe_nn… Prepro… rmse    0.547  0.0309    10 recipe       bag_…     1\n2 camels_recipe_nn… Prepro… rsq     0.787  0.0267    10 recipe       bag_…     1\n3 camels_recipe_rf… Prepro… rmse    0.565  0.0253    10 recipe       rand…     2\n4 camels_recipe_rf… Prepro… rsq     0.770  0.0264    10 recipe       rand…     2\n5 camels_recipe_lm… Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 camels_recipe_lm… Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 camels_recipe_xg… Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 camels_recipe_xg… Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\n\n##Which of the 4 models would you move forward with? ###I will move forward with the neural network model as it performed best in cross-validation.\n#Question 4a: Data Prep / Data Splitting ###Borrowing from the workflow presented above, build your own complete ML pipeline to predict mean streamflow using the CAMELS dataset. A successful model will have a R-squared value &gt; 0.9.\n\nset.seed(13)\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n##Data Spliting ###Set a seed for reproducible Create an initial split with 75% used for training and 25% for testing Extract your training and testing sets Build a 10-fold CV dataset as well\n\ncamels_split_4 &lt;- initial_split(camels, prop = 0.75)\ncamels_train_4 &lt;- training(camels_split_4)\ncamels_test_4  &lt;- testing(camels_split_4)\n\ncamels_cv_4 &lt;- vfold_cv(camels_train_4, v = 10)\n\n#Question 4b: Recipe ###Define a formula you want to use to predict logQmean\n##Describe in words why you are choosing the formula you are. Consult the downloaded PDF.\n######I chose aridity and p_mean as predictors based the CAMELS showing that precipitation and aridity directly influence streamflow.\n##for the data to help you make this decision. Build a recipe that you feel handles the predictors chosen well\n\nrec_4 &lt;- recipe(logQmean ~ aridity + p_mean, data = camels_train_4) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n#Question 4c: Define 3 models ###Define a random forest model using the rand_forest function Set the engine to ranger and the mode to regression Define two other models of your choice\n\nlm_model_4 &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\nrf_model_4 &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n###I need a thrid model… ###I will move forward with recipe_nnet_model was the most accurate in Q3, so I will use it here.\n\nnnet_model_4 &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n#Question 4d: workflow set ###Create a workflow object Add the recipe Add the model(s) Fit the model to the resamples\n\nwf_set_4 &lt;- workflow_set(\n  preproc = list(camels_recipe_4 = rec_4),  # named recipe\n  models = list(\n    lm_model   = lm_model_4,\n    rf_model   = rf_model_4,\n    nnet_model = nnet_model_4\n  )\n) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv_4)\n\n#Question 4e: Evaluation ###Use autoplot and rank_results to compare the models. Describe what model you think is best and why!\n\nrank_results(wf_set_4, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 camels_recipe_4_… Prepro… rmse    0.543  0.0304    10 recipe       bag_…     1\n2 camels_recipe_4_… Prepro… rsq     0.783  0.0252    10 recipe       bag_…     1\n3 camels_recipe_4_… Prepro… rmse    0.551  0.0323    10 recipe       rand…     2\n4 camels_recipe_4_… Prepro… rsq     0.772  0.0280    10 recipe       rand…     2\n5 camels_recipe_4_… Prepro… rmse    0.570  0.0339    10 recipe       line…     3\n6 camels_recipe_4_… Prepro… rsq     0.762  0.0253    10 recipe       line…     3\n\n\n\nautoplot(wf_set_4)\n\n\n\n\n\n\n\n\n\n\nAs we can see, camels_recipe_4_nnet_model is the best model again in terms of rsq.\n#Question 4f: Extract and Evaluate ###Build a workflow (not workflow set) with your favorite model, recipe, and training data Use fit to fit all training data to the model Use augment to make predictions on the test data Create a plot of the observed vs predicted values with clear title, axis labels, and a compelling color scale Describe what you think of the results!\n\nfinal_nnet_wf_4 &lt;- extract_workflow(wf_set_4, \"camels_recipe_4_nnet_model\")\nfinal_nnet_wf_4 &lt;- final_nnet_wf_4 %&gt;%\n  fit(data = camels_train_4)\n\nnnet_test_preds_4 &lt;- augment(final_nnet_wf_4, new_data = camels_test_4)\n\n# View metrics\nmetrics(nnet_test_preds_4, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.616\n2 rsq     standard       0.743\n3 mae     standard       0.360\n\n\n\nggplot(nnet_test_preds_4, aes(x = logQmean, y = .pred, color = aridity)) +\n  geom_point(alpha = 0.7) +\n  geom_abline(linetype = \"dashed\", color = \"red\") +\n  scale_color_viridis_c(trans = \"log\") +\n  theme_linedraw() +\n  labs(\n    title = \"Neural Net: Observed vs Predicted Log Mean Flow\",\n    x = \"Observed Log Mean Flow\",\n    y = \"Predicted Log Mean Flow\",\n    color = \"Aridity (log scale)\"\n  )\n\n\n\n\n\n\n\n\n###I think the results are slighlty scattered but far ess and more evenly spaced than the other prediction models graphed."
  }
]